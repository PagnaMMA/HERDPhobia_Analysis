{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understanding the Dataset and Task\n",
    "The HERDPhobia dataset is organized with three splits:\n",
    "\n",
    "- Training set: Used to train the model\n",
    "- Development (dev) set: Used for validation during training and hyperparameter tuning\n",
    "- Test set: Used only for final evaluation\n",
    "\n",
    "The HERDPhobia dataset is specifically about:\n",
    "\n",
    "Detecting hate speech against Fulani herdsmen in Nigeria\n",
    "Classification into 3 categories:\n",
    "\n",
    "- Hate speech (HT)\n",
    "- Non-hate speech (NHT)\n",
    "- Indeterminate (IND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Environment Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# !pip install transformers datasets pandas numpy scikit-learn torch\n",
    "\n",
    "# Clone the repository\n",
    "#!git clone https://github.com/hausanlp/HERDPhobia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 13:29:13.822982: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-04 13:29:13.871317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import of important library\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load and explore all three splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3090\n",
      "Development set size: 441\n",
      "Test set size: 884\n",
      "\n",
      "Class distribution in training set:\n",
      "label\n",
      "negative    2466\n",
      "positive     624\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in dev set:\n",
      "label\n",
      "negative    352\n",
      "positive     89\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in test set:\n",
      "label\n",
      "negative    705\n",
      "positive    179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label values: ['negative' 'positive']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a955cbc8-35f8-4429-9790-8d98d3522b6d",
       "rows": [
        [
         "0",
         "its is left to be seen if delta people will be annoyed at their politicians for throwing away ¬£m to protect man lets see if they will chide their govt for the loss or as usual attack buhari or fulani as usual for their loss then wed know if they truly deserve d money",
         "negative"
        ],
        [
         "1",
         "hausa language still remains the most populous language spoken by nearly half of the inhabitants of west african sub region its the second widely spoken language in africa next to swahili fulani i think is the third most spoken language in africa",
         "negative"
        ],
        [
         "2",
         "bokoharam herdsmen etc their days are all gone will soon pass too",
         "negative"
        ],
        [
         "3",
         "you didnt do jack i said prove and youre telling me you did your research what does the research point to who is asking you if youre a yoruba or fulani man thats immaterial that insinuation is for the po supporters not me",
         "negative"
        ],
        [
         "4",
         "not a sector can boldly say this is one good thing buhari has done well i think you were voted in so nigerians can learn and wakeup i just hope it wont be too late for us cause its likely this buhari will turn everything upside down fulani igboho bandits yorubas",
         "negative"
        ],
        [
         "5",
         "seriki fulani i am always scared till now when i heard the name sunday igboho",
         "negative"
        ],
        [
         "6",
         "people come from a tribe you can be critical of choices its unfair to call out yoruba hausa fulani or igbo when a whole may or may not be conscious of why they are being judged",
         "negative"
        ],
        [
         "7",
         "because america is hospitable d whole world is going there to buy into it also because lagos is hospitable the whole world including igbo are coming to buy into it remember d biggest refinery is now in lagos own by fulani man who is buying into ur own land",
         "negative"
        ],
        [
         "8",
         "pls stop this herdsmen youve been talking were you at the church to made you believe its herdsmen why not wait and let security investigate it",
         "negative"
        ],
        [
         "9",
         "dont mind this fulani man",
         "negative"
        ],
        [
         "10",
         "where are the arewa youth when the soldiers are escorting fulani herdsmen to ogun state to harass and molest people in their homes where are they now this marriage cannot work lets stop deceiving ourselves",
         "positive"
        ],
        [
         "11",
         "you are wrong sunday igboho only asked fulani killer herdsmen to stop killing our people on their farms though there are colorations of secession in his demands i didnt see a crime in that",
         "negative"
        ],
        [
         "12",
         "abeg can someone make this make sense to me how them take they arrest cow now where d hand cups naija dey automatic cruise",
         "negative"
        ],
        [
         "13",
         "suspected herdsmen kidnap two absu lecturers",
         "negative"
        ],
        [
         "14",
         "what do you expect of a man that works for leaders that behaves like cows keyamo himself is a cow",
         "positive"
        ],
        [
         "15",
         "the atyap fulani and hausa communities in zangonkataf local government area of kaduna state on saturday held a successful peace and reconciliation summit participants in the summit condemned the killings and destruction that have occurred and resolved",
         "negative"
        ],
        [
         "16",
         "d fulanisationislamisation rhetoric is a ruse its a disguise of their frustration with apc they will vote a fulani moslem from pdp tomorrow apc is their nemesis they demoniseattack fulani herders and call for boycott of beef while eating suya with trophy",
         "positive"
        ],
        [
         "17",
         "so after all the ordeals you and yours passed through in the hands of fulani mauruders youre still supporting another fulani man to become the president you see why i find it hard to sympathize with people who do not learn from their past cry cry baby",
         "positive"
        ],
        [
         "18",
         "gaskiya yanzu kam na yarda you are a pro to some naga she is sooo convinced about it anyways ive got some new fulani words to teach you interested",
         "negative"
        ],
        [
         "19",
         "absolutely right yes not all herdsmen are criminals but we must be able to account for all residents in our forests where by the way most of these crimes take place",
         "negative"
        ],
        [
         "20",
         "im waiting to see where you call fulani herdsmen terrorists",
         "negative"
        ],
        [
         "21",
         "if we had guns as citizens in nigeria boko haram and fulani herdsmen wont have been able to carry out their current genocide",
         "negative"
        ],
        [
         "22",
         "just posted a photo lagos island",
         "negative"
        ],
        [
         "23",
         "abaribe that went to bail mazi kanu has suddenly jumped to support a fulani atiku without protesting the south east should be given pdp ticket",
         "negative"
        ],
        [
         "24",
         "u think nigeria have the moral right to advice it citizen abt danger in a foreign country even if we want citizen to come back home bcos of danger over there where do you want them to come to to come back and face bh banditsfulani herdmen or another level of police brutality",
         "negative"
        ],
        [
         "25",
         "my disappointment as a nigerian continuesly increasing daily as a fulani nigerian soldiers guilding chinese in lagos broke a bus drivers whole front screenside mirrors and equally burst both tyres reasonwhy should passenger talk in the moving bus",
         "negative"
        ],
        [
         "26",
         "god bless u man u just say my mind this people will just let victims of go in vain we never learn anything ooni that have been hearing fulani killing rapping and kidnapping people that did nothing significantly about it now someone challenged him",
         "negative"
        ],
        [
         "27",
         "did a quick summary of the average occupancy rate of the knight towers project and the attached breakdown flier says it all cash back guaranteed like never before dm before it get sold bbnaija tinubu igbo fulani nigerians ipob",
         "negative"
        ],
        [
         "28",
         "hi and good evening i dont know how to get this out in the world with our great artists nigerian music industry i mean the happenings in our societies is not captured in their music theme the kidnaping killings attacks fulani herbmens bandits injustice",
         "negative"
        ],
        [
         "29",
         "when a fulani man tell u the place ur going isnt far dont believe him because u will end up walking twice the distance",
         "negative"
        ],
        [
         "30",
         "na fulani go answer this one",
         "negative"
        ],
        [
         "31",
         "this is how they roll quite frankly they dont give a damn every security position must be occupied by a or a all of you sophisticated morons grammarians and multipurpose efulefus from the south yapping about one nigeria your shame is shaming me",
         "positive"
        ],
        [
         "32",
         "some politicians know d truth behind the herdsmen migration down south but choose to add tribal religious n ethnic colours to it for their selfish political gains while orders are plainly ignorant of d effect of global warming in forced migration",
         "negative"
        ],
        [
         "33",
         "dont follow the herd you may be mistaken for a cow",
         "positive"
        ],
        [
         "34",
         "fulani herdsmen soldiers kill militia herders in benue nasarawa taraba",
         "negative"
        ],
        [
         "35",
         "unrepentant fulani slave and worse still its mental slavery which is almost incurable",
         "positive"
        ],
        [
         "36",
         "i tweeted earlier today that there will be resistance from the northern forces against the decision of akeredolu to ban herdsmen in ondo state this evening garba shehu faulted the ban on behalf of the presidency this is the time for the yoruba nation to stand united in voice",
         "negative"
        ],
        [
         "37",
         "so bola tinubu have money to donate when people were killed in igangan he do nothing when people were killed in in his own state he was saying we should ask those people questions haba yoruba fulani amotekun kogi",
         "negative"
        ],
        [
         "38",
         "can you please explain how youre fulaniondo im one funny fulani too tho",
         "negative"
        ],
        [
         "39",
         "livestock technology and the connected cow everything in the digital age is connected including farming and agriculture livestock technology can enhance or improve the productivity capacity welfare or management of animals and livestock",
         "negative"
        ],
        [
         "40",
         "this must be some governors people hiding under the fulani group tag for their selfish interest",
         "negative"
        ],
        [
         "41",
         "they should go to zambisa forest crazy people or they should go after the fulani herdmen",
         "positive"
        ],
        [
         "42",
         "this is a lie a dangerous one tiv militia are disguising as herdsmen to kill and terrorize their own communities in benue some have been apprehended stop spreading fake news",
         "negative"
        ],
        [
         "43",
         "and a lot of this shit has been done here in the name of fulani until we stop inciting violence and start investigating the real perpetrators of evil crime nigeria is far away from unity",
         "negative"
        ],
        [
         "44",
         "the youth is probably fulani and has taken the job of a more decent betterqualified person so boo hoo",
         "negative"
        ],
        [
         "45",
         "how long will we continue being cruelly killed for simply refusing to surrender our farms for fulani cattle to feed on what is our crime that our cries no matter how loud wont be heard more than were killed today how many tomorrow",
         "negative"
        ],
        [
         "46",
         "who is pointing a knife to a moron and senseless idiot like you i am from abia state i am angry at the wanton killings perpetrated by enemies of nigeria the fulani herdsmen etc i only said let lobe lead because even the bible asked us to love our enemies the next response",
         "positive"
        ],
        [
         "47",
         "this deadly fulani terrorist gunment could smuggle in a boko haram leader camouflagehim with reality face surgery mask to commence catastrophic war in africa threat to millions of african lives childish thuggery in aso rock abuja puts all in grave danger",
         "positive"
        ],
        [
         "48",
         "in nigeria today buhari is wasting nigeria resources to ensure his people fulanis has home in every state in nigeria still southern politicians see nothing wrong on his quest",
         "positive"
        ],
        [
         "49",
         "ka xama dan fulani kawai",
         "negative"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3090
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its is left to be seen if delta people will be...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hausa language still remains the most populous...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bokoharam herdsmen etc their days are all gone...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you didnt do jack i said prove and youre telli...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not a sector can boldly say this is one good t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>if law enforcement agents arrest sunday govt w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>bought fuel pms at n this evening feeling so b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>the governor commends officers and men of oper...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <td>fulanis herdsmen are taking over benue y·∫πn y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>he has since apologised and apology has been a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3090 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label\n",
       "0     its is left to be seen if delta people will be...  negative\n",
       "1     hausa language still remains the most populous...  negative\n",
       "2     bokoharam herdsmen etc their days are all gone...  negative\n",
       "3     you didnt do jack i said prove and youre telli...  negative\n",
       "4     not a sector can boldly say this is one good t...  negative\n",
       "...                                                 ...       ...\n",
       "3085  if law enforcement agents arrest sunday govt w...  negative\n",
       "3086  bought fuel pms at n this evening feeling so b...  negative\n",
       "3087  the governor commends officers and men of oper...  negative\n",
       "3088  fulanis herdsmen are taking over benue y·∫πn y...  positive\n",
       "3089  he has since apologised and apology has been a...  positive\n",
       "\n",
       "[3090 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset_path = \"data\"\n",
    "train_path = os.path.join(dataset_path, \"train.tsv\")\n",
    "dev_path = os.path.join(dataset_path, \"dev.tsv\")\n",
    "test_path = os.path.join(dataset_path, \"test.tsv\")\n",
    "\n",
    "# Read the data\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Explore the data\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Development set size: {len(dev_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Check class distribution in all splits\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"\\nClass distribution in dev set:\")\n",
    "print(dev_df['label'].value_counts())\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "# Check for label encoding (if labels are categorical or numerical)\n",
    "print(\"\\nLabel values:\", train_df['label'].unique())\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataset to a format suitable for Hugging Face's datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3090\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 441\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 884\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict with all three splits\n",
    "herdphobia_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': dev_dataset,  # Using 'validation' as it's the standard name in Hugging Face\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(herdphobia_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection and Fine-tuning Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select an appropriate Afrocentric PLM for Hausa language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student24/anaconda3/envs/data_science/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64010108e5ac4425a7a19ea83cc04b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cefd49252042acb68086a26bd83014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c142a038c27e41d5bc7188a60b83560c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584d163b652d4b68a2e0719b4305b6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Choose an Afrocentric PLM that supports Hausa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#model_name = \"masakhane/afroxml_large\"  # AfroXLMR\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcastorini/afriberta_large\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# AfriBerta\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n",
      "File \u001b[0;32m~/anaconda3/envs/data_science/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:855\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/data_science/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2086\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2083\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2084\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/data_science/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2325\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2330\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/data_science/lib/python3.12/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:155\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    141\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m ):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n",
      "File \u001b[0;32m~/anaconda3/envs/data_science/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "# Choose an Afrocentric PLM that supports Hausa\n",
    "#model_name = \"masakhane/afroxml_large\"  # AfroXLMR\n",
    "model_name = \"castorini/afriberta_large\" # AfriBerta\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize all splits\n",
    "tokenized_datasets = herdphobia_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare for training\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1  Updated Metrics Function for 3-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics function for multi-class classification\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Multi-class metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Detailed classification report\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'class_0_f1': report['0']['f1-score'],  # HT class\n",
    "        'class_1_f1': report['1']['f1-score'],  # NHT class\n",
    "        'class_2_f1': report['2']['f1-score']   # IND class\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's leverage the dev set for validation during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(f\"Test set results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Performance Improvement Strategies for Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's class imbalance\n",
    "train_label_counts = train_df['label'].value_counts()\n",
    "print(\"Label distribution:\", train_label_counts)\n",
    "\n",
    "# Calculate class weights if needed\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Apply class weights in the model\n",
    "# Note: You would typically do this inside the model's forward pass\n",
    "# But with Hugging Face, we need a custom training loop or a custom model\n",
    "# For simplicity, we'll use weighted data augmentation instead\n",
    "\n",
    "# Weighted data augmentation for minority classes\n",
    "def create_balanced_augmented_dataset(df, class_counts):\n",
    "    # Find the majority class count\n",
    "    max_count = class_counts.max()\n",
    "    \n",
    "    # Create dataframes for each class\n",
    "    class_dfs = {}\n",
    "    for class_label in class_counts.index:\n",
    "        class_dfs[class_label] = df[df['label'] == class_label]\n",
    "    \n",
    "    # Augment each class to reach similar size as majority class\n",
    "    augmented_dfs = []\n",
    "    for class_label, class_df in class_dfs.items():\n",
    "        if len(class_df) < max_count:\n",
    "            # Calculate how many times to duplicate\n",
    "            augment_factor = int(np.ceil(max_count / len(class_df))) - 1\n",
    "            \n",
    "            # Apply augmentation\n",
    "            augmented_class_df = class_df.copy()\n",
    "            for _ in range(augment_factor):\n",
    "                # Apply text augmentation to the minority class\n",
    "                augmented_texts = []\n",
    "                augmented_labels = []\n",
    "                \n",
    "                for _, row in class_df.iterrows():\n",
    "                    # Apply simple augmentation (you can use the techniques defined earlier)\n",
    "                    # For now, just duplicate the example\n",
    "                    augmented_texts.append(row['text'])\n",
    "                    augmented_labels.append(row['label'])\n",
    "                \n",
    "                temp_df = pd.DataFrame({\n",
    "                    'text': augmented_texts,\n",
    "                    'label': augmented_labels\n",
    "                })\n",
    "                \n",
    "                augmented_class_df = pd.concat([augmented_class_df, temp_df], ignore_index=True)\n",
    "            \n",
    "            # Take only what we need to balance\n",
    "            augmented_class_df = augmented_class_df.sample(max_count, replace=False)\n",
    "            augmented_dfs.append(augmented_class_df)\n",
    "        else:\n",
    "            augmented_dfs.append(class_df)\n",
    "    \n",
    "    # Combine all balanced classes\n",
    "    balanced_df = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    return balanced_df\n",
    "\n",
    "# Create a more balanced dataset\n",
    "balanced_train_df = create_balanced_augmented_dataset(train_df, train_label_counts)\n",
    "print(f\"Original training set size: {len(train_df)}\")\n",
    "print(f\"Balanced training set size: {len(balanced_train_df)}\")\n",
    "print(\"New class distribution:\", balanced_train_df['label'].value_counts())\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "balanced_train_dataset = Dataset.from_pandas(balanced_train_df)\n",
    "balanced_dataset_dict = DatasetDict({\n",
    "    'train': balanced_train_dataset,\n",
    "    'validation': dev_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Tokenize the balanced dataset\n",
    "tokenized_balanced_datasets = balanced_dataset_dict.map(tokenize_function, batched=True)\n",
    "tokenized_balanced_datasets = tokenized_balanced_datasets.remove_columns([\"text\"])\n",
    "tokenized_balanced_datasets = tokenized_balanced_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_balanced_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Advanced Fine-tuning Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up improved training arguments\n",
    "improved_training_args = TrainingArguments(\n",
    "    output_dir=\"./improved_results\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    warmup_ratio=0.1,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize improved trainer with balanced dataset\n",
    "improved_trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3),\n",
    "    args=improved_training_args,\n",
    "    train_dataset=tokenized_balanced_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_balanced_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train improved model\n",
    "improved_trainer.train()\n",
    "\n",
    "# Evaluate improved model\n",
    "improved_test_results = improved_trainer.evaluate(tokenized_balanced_datasets[\"test\"])\n",
    "print(f\"Improved model results: {improved_test_results}\")\n",
    "\n",
    "# Display per-class performance\n",
    "print(\"Per-class F1 scores:\")\n",
    "print(f\"Hate speech (HT): {improved_test_results['eval_class_0_f1']:.4f}\")\n",
    "print(f\"Non-hate speech (NHT): {improved_test_results['eval_class_1_f1']:.4f}\")\n",
    "print(f\"Indeterminate (IND): {improved_test_results['eval_class_2_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the model\n",
    "test_predictions = improved_trainer.predict(tokenized_balanced_datasets[\"test\"])\n",
    "predicted_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "true_labels = tokenized_balanced_datasets[\"test\"][\"labels\"]\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "class_names = [\"HT\", \"NHT\", \"IND\"]  # Replace with actual class names\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(\"Detailed classification report:\")\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Error Analysis and Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples where the model made mistakes\n",
    "def get_error_examples(test_dataset, true_labels, predicted_labels, n=10):\n",
    "    error_indices = np.where(true_labels != predicted_labels)[0]\n",
    "    \n",
    "    if len(error_indices) == 0:\n",
    "        print(\"No errors found!\")\n",
    "        return\n",
    "    \n",
    "    # Select a random sample of errors (up to n)\n",
    "    sample_size = min(n, len(error_indices))\n",
    "    selected_indices = np.random.choice(error_indices, size=sample_size, replace=False)\n",
    "    \n",
    "    # Get the original text and labels\n",
    "    error_examples = []\n",
    "    for idx in selected_indices:\n",
    "        example = {\n",
    "            'text': test_df.iloc[idx]['text'],\n",
    "            'true_label': class_names[true_labels[idx]],\n",
    "            'predicted_label': class_names[predicted_labels[idx]]\n",
    "        }\n",
    "        error_examples.append(example)\n",
    "    \n",
    "    return error_examples\n",
    "\n",
    "error_examples = get_error_examples(test_df, true_labels, predicted_labels)\n",
    "\n",
    "print(\"Error Analysis:\")\n",
    "for i, example in enumerate(error_examples):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Text: {example['text']}\")\n",
    "    print(f\"True label: {example['true_label']}\")\n",
    "    print(f\"Predicted label: {example['predicted_label']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Publishing to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's publish our best model to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "notebook_login()\n",
    "\n",
    "# Choose the best model\n",
    "best_trainer = improved_trainer\n",
    "\n",
    "# Create a model card\n",
    "model_card = \"\"\"\n",
    "---\n",
    "language: hau\n",
    "license: mit\n",
    "datasets:\n",
    "  - hausanlp/HERDPhobia\n",
    "tags:\n",
    "  - hate-speech-detection\n",
    "  - hausa\n",
    "  - african-nlp\n",
    "  - fulani-herdsmen\n",
    "---\n",
    "\n",
    "# HERDPhobia Hate Speech Detection Model\n",
    "\n",
    "This model is fine-tuned on the HERDPhobia dataset for detecting hate speech against Fulani herdsmen in Nigeria in Hausa language text.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model is based on [masakhane/afroxml-r](https://huggingface.co/masakhane/afroxml-r) fine-tuned on the HERDPhobia dataset. It performs 3-class classification:\n",
    "- Hate speech (HT)\n",
    "- Non-hate speech (NHT)\n",
    "- Indeterminate (IND)\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This model is intended for detecting hate speech against Fulani herdsmen in Hausa language text from social media and other online sources.\n",
    "\n",
    "## Training Data\n",
    "\n",
    "The model was trained on the HERDPhobia dataset, which contains annotated tweets in Hausa.\n",
    "\n",
    "## Performance\n",
    "\n",
    "The model achieves the following scores on the test set:\n",
    "- Overall Accuracy: {improved_test_results['eval_accuracy']:.4f}\n",
    "- Overall F1 Score: {improved_test_results['eval_f1']:.4f}\n",
    "- Per-class F1 scores:\n",
    "  - Hate speech (HT): {improved_test_results['eval_class_0_f1']:.4f}\n",
    "  - Non-hate speech (NHT): {improved_test_results['eval_class_1_f1']:.4f}\n",
    "  - Indeterminate (IND): {improved_test_results['eval_class_2_f1']:.4f}\n",
    "\n",
    "## Improvement Strategies\n",
    "\n",
    "1. Class balancing techniques were applied to handle uneven class distribution\n",
    "2. Hyperparameter optimization improved overall performance\n",
    "3. Error analysis was conducted to understand model limitations\n",
    "\n",
    "## Limitations\n",
    "\n",
    "This model may have limitations in detecting subtle forms of hate speech or hate speech expressed in dialects or slang not well-represented in the training data.\n",
    "\"\"\"\n",
    "\n",
    "# Save the model card\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "model_name_on_hub = \"YOUR_USERNAME/hausa-herdphobia-classifier\"  # Replace with your username\n",
    "\n",
    "best_trainer.push_to_hub(\n",
    "    repo_id=model_name_on_hub,\n",
    "    commit_message=\"Add fine-tuned model for Hausa hate speech detection against Fulani herdsmen\"\n",
    ")\n",
    "\n",
    "print(f\"Model successfully pushed to {model_name_on_hub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion and Analysis Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
